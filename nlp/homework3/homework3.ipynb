{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo.png](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/gazeta_train.jsonl'\n",
    "vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, date, url, summary, title, text):\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.summary = summary\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "\n",
    "\n",
    "def parse_datetime(string):\n",
    "  return datetime.strptime(string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def parse_article_json(obj):\n",
    "    date = parse_datetime(obj['date'])\n",
    "    url = obj['url']\n",
    "    summary = obj['summary']\n",
    "    title = obj['title']\n",
    "    text = obj['text']\n",
    "    return Article(date, url, summary, title, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_gazeta(text):\n",
    "    lines = text.split('\\n')\n",
    "    articles = [parse_article_json(json.loads(line)) for line in lines if line != '']\n",
    "    return articles\n",
    "\n",
    "\n",
    "def load_and_parse_gazeta(path):\n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    return parse_gazeta(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = load_and_parse_gazeta(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_stopwords = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = text\n",
    "    new_text = new_text.lower()  # Привести к нижнему регистру\n",
    "    new_text = re.sub(r'[^\\w\\s]', ' ', new_text)  # Убрать пунктуацию\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in ru_stopwords]  # Убрать стоп слова\n",
    "    # tokens = [lemmatize_word(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_article_body(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    sents = [preprocess_text(sent) for sent in sents]\n",
    "    sents_words = [tokenize_sentence(sent) for sent in sents]\n",
    "    return sents_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for article in articles[:1000]:\n",
    "    sentences += tokenize_article_body(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сегодня',\n",
       " 'транспортный',\n",
       " 'налог',\n",
       " 'начисляется',\n",
       " 'зависимости',\n",
       " 'мощности',\n",
       " 'автомобиля',\n",
       " 'причем',\n",
       " 'цена',\n",
       " 'сильных',\n",
       " 'машин',\n",
       " 'выше',\n",
       " 'малолитражек']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эмбединг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FastText(sentences=sentences, vector_size=vector_size, min_count=1, window=5, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация + Word2Vec очень медленнно, FastText тоже не быстрый. А тут одно из двух, поэтому я выбираю третий вариант: предобученная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "\n",
    "path = 'data/navec_news_v1_1B_250K_300d_100q.tar'\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37482268287720866, 0.9287834559341686, 0.34254276976931297)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(request):\n",
    "    tokens = tokenize_text(request)\n",
    "    vector = np.zeros((vector_size))\n",
    "    num_tokens = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word in navec:\n",
    "            vector += navec[word]\n",
    "    vector /= num_tokens\n",
    "    return vector\n",
    "\n",
    "\n",
    "distance.cosine(vectorize('взятку'), vectorize('Российский чиновник дал взятку')), \\\n",
    "    distance.cosine(vectorize('взятку'), vectorize('Хакеры взломали пентагон')), \\\n",
    "        distance.cosine(vectorize('взятку'), vectorize('взятки'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vectors(text):\n",
    "    sents = tokenize_article_body(text)\n",
    "    vecs = []\n",
    "    for sent in sents:\n",
    "        sent_vec = np.zeros((vector_size))\n",
    "        num_tokens = 0\n",
    "        for token in sent:\n",
    "            if token in navec:\n",
    "                sent_vec += navec[token]\n",
    "                num_tokens+=1\n",
    "        if num_tokens > 0:\n",
    "            sent_vec /= num_tokens\n",
    "        # sent_vec /= np.linalg.norm(sent_vec)\n",
    "        vecs.append(sent_vec)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:subsample_size]:\n",
    "    article.summary_vecs = text_to_vectors(article.summary)\n",
    "    article.title_vecs = text_to_vectors(article.title)\n",
    "    article.sentences = nltk.sent_tokenize(article.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ближайшие соседи и поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = annoy.AnnoyIndex(vector_size, 'angular')\n",
    "counter = 0\n",
    "for article in articles[:subsample_size]:\n",
    "    for vec in article.summary_vecs:\n",
    "        index.add_item(counter, vec)\n",
    "    for vec in article.title_vecs:\n",
    "        index.add_item(counter, vec)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.build(10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenize_sentence(preprocess_text(text))\n",
    "\n",
    "\n",
    "def find_nearest_sentence(article, request):\n",
    "    ind = 0\n",
    "    min_dist = 1000000\n",
    "    for i in range(len(article.summary_vecs)):\n",
    "        dist = distance.cosine(article.summary_vecs[i], request)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            ind = i\n",
    "    return ind\n",
    "\n",
    "\n",
    "def search(request):\n",
    "    tokens = tokenize_text(request)\n",
    "    vector = np.zeros((vector_size))\n",
    "    num_tokens = len(tokens)\n",
    "    for word in tokens:\n",
    "        if word in navec:\n",
    "            vector += navec[word]\n",
    "    if num_tokens > 0:\n",
    "        vector /= num_tokens\n",
    "        # vector /= np.linalg.norm(vector)\n",
    "    results = index.get_nns_by_vector(vector, 5,)\n",
    "    return [(i, articles[i].title, \\\n",
    "        articles[i].sentences[find_nearest_sentence(articles[i], vector)]) \\\n",
    "        for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(273,\n",
       "  'Убийца не был педофилом',\n",
       "  'В Екатеринбурге к пожизненному сроку приговорен Сергей Кутнюк, которого за убийства и изнасилования детей милиционеры разыскивали шесть лет.'),\n",
       " (390,\n",
       "  'С убийства Маркелова снят мотив',\n",
       "  'С предполагаемого убийцы адвоката Станислава Маркелова и журналистки Анастасии Бабуровой сняты обвинения в убийстве антифашиста Александра Рюхина в 2006 году.'),\n",
       " (45,\n",
       "  'Таксист стрелял без остановки',\n",
       "  'Позже полиция обнаружила в лесу и труп предполагаемого преступника — он застрелился.'),\n",
       " (162,\n",
       "  'Пациент убил врача в упор',\n",
       "  'В Пущинском научном центре РАН пациент застрелил из ружья заместителя главврача больницы, а затем покончил с собой.'),\n",
       " (725,\n",
       "  'Убийцу опознали по прорезям для глаз',\n",
       "  'Генерал Сергей Кизюн, сидевший в машине Руслана Ямадаева в момент его убийства, опознал стрелявшего в одном из подсудимых.')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('убийца')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(819,\n",
       "  'Шпионский кодер',\n",
       "  'Доказательств того, что он был связан со шпионской сетью, у следствия так и не появилось.'),\n",
       " (777,\n",
       "  'Шпионский Роман',\n",
       "  'Арест российских шпионов в конце июня спровоцировал звонок Анны Чепмен в Москву.'),\n",
       " (77,\n",
       "  'Брайант сыграл для Спилберга',\n",
       "  'На игру в лос-анджелесский «Стейпл Центр» вместе с рядовыми болельщиками пришли немало звезд Голливуда, начиная Стивеном Спилбергом и заканчивая Шарлиз Терон и Крисом Роком.'),\n",
       " (80,\n",
       "  'Фильтруй коммент',\n",
       "  'Интернет-СМИ должны редактировать комментарии своих читателей.'),\n",
       " (81,\n",
       "  'Фильтруй коммент',\n",
       "  'Интернет-СМИ должны редактировать комментарии своих читателей.')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('шпионский кодер')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3af7e179898fb99d78969cca94508015726394a06defd0be323cbf435423c41d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('nn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
