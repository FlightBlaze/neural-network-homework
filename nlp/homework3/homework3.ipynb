{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Обновление:__ Использование Max вместо Mean для объединения эмбедингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo.png](logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/gazeta_train.jsonl'\n",
    "vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, date, url, summary, title, text):\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.summary = summary\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "\n",
    "\n",
    "def parse_datetime(string):\n",
    "  return datetime.strptime(string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def parse_article_json(obj):\n",
    "    date = parse_datetime(obj['date'])\n",
    "    url = obj['url']\n",
    "    summary = obj['summary']\n",
    "    title = obj['title']\n",
    "    text = obj['text']\n",
    "    return Article(date, url, summary, title, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_gazeta(text):\n",
    "    lines = text.split('\\n')\n",
    "    articles = [parse_article_json(json.loads(line)) for line in lines if line != '']\n",
    "    return articles\n",
    "\n",
    "\n",
    "def load_and_parse_gazeta(path):\n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    return parse_gazeta(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = load_and_parse_gazeta(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_stopwords = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = text\n",
    "    new_text = new_text.lower()  # Привести к нижнему регистру\n",
    "    new_text = re.sub(r'[^\\w\\s]', ' ', new_text)  # Убрать пунктуацию\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in ru_stopwords]  # Убрать стоп слова\n",
    "    # tokens = [lemmatize_word(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_article_body(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    sents = [preprocess_text(sent) for sent in sents]\n",
    "    sents_words = [tokenize_sentence(sent) for sent in sents]\n",
    "    return sents_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for article in articles[:1000]:\n",
    "    sentences += tokenize_article_body(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сегодня',\n",
       " 'транспортный',\n",
       " 'налог',\n",
       " 'начисляется',\n",
       " 'зависимости',\n",
       " 'мощности',\n",
       " 'автомобиля',\n",
       " 'причем',\n",
       " 'цена',\n",
       " 'сильных',\n",
       " 'машин',\n",
       " 'выше',\n",
       " 'малолитражек']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эмбединг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft = FastText(sentences=sentences, vector_size=vector_size, min_count=1, window=5, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация + Word2Vec очень медленнно, FastText тоже не быстрый. А тут одно из двух, поэтому я выбираю третий вариант: предобученная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "\n",
    "path = 'data/navec_news_v1_1B_250K_300d_100q.tar'\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5989381968975067, 0.9491334371268749, 0.342542827129364)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(request):\n",
    "    tokens = tokenize_text(request)\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in navec:\n",
    "            vectors.append(navec[word])\n",
    "    vec_stack = np.stack(vectors)\n",
    "    vector = np.max(vec_stack, axis=0)\n",
    "    return vector\n",
    "\n",
    "\n",
    "distance.cosine(vectorize('взятку'), vectorize('Российский чиновник дал взятку')), \\\n",
    "    distance.cosine(vectorize('взятку'), vectorize('Хакеры взломали пентагон')), \\\n",
    "        distance.cosine(vectorize('взятку'), vectorize('взятки'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.5989381968975067, 0.9491334371268749, 0.342542827129364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vectors(text):\n",
    "    sents = tokenize_article_body(text)\n",
    "    vecs = []\n",
    "    for sent in sents:\n",
    "        sent_vecs = []\n",
    "        for token in sent:\n",
    "            if token in navec:\n",
    "                sent_vecs.append(navec[token])\n",
    "        vector = np.zeros((vector_size))\n",
    "        if len(sent_vecs) > 0:\n",
    "            vec_stack = np.stack(sent_vecs)\n",
    "            vector = np.max(vec_stack, axis=0)\n",
    "        vecs.append(vector)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:subsample_size]:\n",
    "    article.summary_vecs = text_to_vectors(article.summary)\n",
    "    article.title_vecs = text_to_vectors(article.title)\n",
    "    article.sentences = nltk.sent_tokenize(article.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ближайшие соседи и поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = annoy.AnnoyIndex(vector_size, 'angular')\n",
    "counter = 0\n",
    "for article in articles[:subsample_size]:\n",
    "    for vec in article.summary_vecs:\n",
    "        index.add_item(counter, vec)\n",
    "    for vec in article.title_vecs:\n",
    "        index.add_item(counter, vec)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.build(10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenize_sentence(preprocess_text(text))\n",
    "\n",
    "\n",
    "def find_nearest_sentence(article, request):\n",
    "    ind = 0\n",
    "    min_dist = 1000000\n",
    "    for i in range(len(article.summary_vecs)):\n",
    "        dist = distance.cosine(article.summary_vecs[i], request)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            ind = i\n",
    "    return ind\n",
    "\n",
    "\n",
    "def search(request):\n",
    "    tokens = tokenize_text(request)\n",
    "    vector = np.zeros((vector_size))\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in navec:\n",
    "            vectors.append(navec[word])\n",
    "    if len(vectors) > 0:\n",
    "            vec_stack = np.stack(vectors)\n",
    "            vector = np.max(vec_stack, axis=0)\n",
    "        # vector /= np.linalg.norm(vector)\n",
    "    results = index.get_nns_by_vector(vector, 5,)\n",
    "    return [(i, articles[i].title, \\\n",
    "        articles[i].sentences[find_nearest_sentence(articles[i], vector)]) \\\n",
    "        for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(65,\n",
       "  'Путин прокололся',\n",
       "  'Mercedes, в котором находился Владимир Путин, попал в ДТП.'),\n",
       " (724,\n",
       "  'Где народ?',\n",
       "  'Система работает плохо и с перегибами, требуется понять, что именно о гражданах нужно знать органам власти, гласит заявка на научное исследование, заказанное администрацией президента.'),\n",
       " (733,\n",
       "  'Медведеву подарили Путина',\n",
       "  'Президент России Дмитрий Медведев посетил форум «Селигер» — это первый визит главы государства за всю историю существования «нашистского» проекта.'),\n",
       " (48,\n",
       "  'OneRepublic прохлопали Москву',\n",
       "  'В Москве выступили американские поп-рокеры OneRepublic — любимцы Тимбаленда вскружили голову своим поклонницам и показали, как работает живьем их машина по зарабатыванию «Грэмми».'),\n",
       " (712,\n",
       "  'О культе Путина',\n",
       "  'С каждым годом все больше россиян видят в России признаки культа личности Владимира Путина.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('путин')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(388,\n",
       "  'Россия не отвечает',\n",
       "  'Россия якобы не ответила ни на один заданный еврочиновниками вопрос, в том числе по делам Магнитского и Ходорковского.'),\n",
       " (615,\n",
       "  'Евро почуял спрос',\n",
       "  'Дальше будет хуже: инвесторы начинают понимать, что их ожидания по экономическому росту были завышенными.'),\n",
       " (276,\n",
       "  'Европа продолжает снижение',\n",
       "  'По мнению экспертов, в дальнейшем спад рынка продолжится в связи с завершением стимулирующих программ и нестабильной экономической ситуацией.'),\n",
       " (430,\n",
       "  'Опухли от кризиса',\n",
       "  'Чудеса по-русски: граждане, согласно официальной статистике, стали жить богаче, чем до кризиса, производство же наверстало кризисные потери едва ли наполовину.'),\n",
       " (893,\n",
       "  '«Денис был просто лучше»',\n",
       "  'В субботу в немецком Шверине российский боксер Денис Лебедев во втором раунде нокаутировал своего соотечественника Александра Алексеева.')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('эксперт')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(440,\n",
       "  'Без комментариев',\n",
       "  'Роскомнадзор вынес первое предупреждение интернет-изданию за комментарии читателей.'),\n",
       " (914,\n",
       "  'Видео придется подождать',\n",
       "  'Вопрос же о видеоповторах должен подняться на заседании совета в октябре.'),\n",
       " (66,\n",
       "  'На свой медстрах и иск',\n",
       "  'Он дает гражданам право на выбор страховщика, лечебного учреждения и врача.'),\n",
       " (388,\n",
       "  'Россия не отвечает',\n",
       "  'Россия якобы не ответила ни на один заданный еврочиновниками вопрос, в том числе по делам Магнитского и Ходорковского.'),\n",
       " (824,\n",
       "  'Послали на три слова',\n",
       "  'Сенаторы отказались принять документ, так как при передаче текста из одной палаты в другую из него исчезли несколько слов.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('комментарий')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3af7e179898fb99d78969cca94508015726394a06defd0be323cbf435423c41d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('nn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
